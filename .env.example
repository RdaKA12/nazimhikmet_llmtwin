MONGO_URL=mongodb://mongo:27017
QDRANT_URL=http://qdrant:6333
QDRANT_COLLECTION=nazim_text_chunks
EMBED_MODEL=intfloat/multilingual-e5-base
CHUNK_SIZE=800
CHUNK_OVERLAP=120

# LLM runtime configuration (local-first per book's guidance)
# Provider can be: ollama | openai_compat
LLM_PROVIDER=ollama

# Default model for local inference
# For 8GB VRAM laptops, a 3B instruct model is a safe default
LLM_MODEL_ID=llama3.2:3b-instruct

# Ollama HTTP API (if using LLM_PROVIDER=ollama)
OLLAMA_API_URL=http://localhost:11434

# OpenAI-compatible base URL (e.g., vLLM server: `python -m vllm.entrypoints.openai.api_server ...`)
OPENAI_COMPAT_URL=http://localhost:8000/v1
OPENAI_API_KEY=

# RAG
TOP_K=5
LLM_MAX_TOKENS=512
